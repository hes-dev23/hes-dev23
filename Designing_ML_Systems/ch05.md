# Feature Engineering
여러 회사들과 연구에서 알맞은 feature를 가지고 있는 것이 ML model을 발전 시킬때 알고리즘을 잘 선택하는 것보다 중요하다고 한다.

## Learned Features Versus Engineered Features

딥러닝으로 feature를 만들어내어 좋은 모델을 만들 수 있지만,
프로덕션 레벨에서 활용되는 ML 어플리케이션들은 주로 deep learning을 활용하는 모델이 아니며 모든 feature를 자동화해서 만들기에는 어려움이 있다.

Feature Engineering은 도메인 특정 기술에 대한 지식을 필요로한다.
그러나 deep learning을 통해 일부 동작만 해주어도 일부 조취를 취한 데이터로 부터 디테일한 작업을 모델에게 기대할 수 있다. 

하지만 모두 그런 케이스에 해당하는 것이 아니라, 댓글의 spam을 막기 위한 ML system등에서는 이외에도 다른 feature들을 필요로 한다.
일부 task들은 수백만가지의 feature를 요구할 수도 있고, 도메인의 특성이 강한 경우 다른 feature들을 활용할 수도 있다.

## Common Feature Engineering Operations

### Handling Missing Values
프로덕션의 데이터를 마주할 때 가장 먼저 만나는 문제 중 하나는 missing value이다. 
하지만 모든 missing value가 같은 유형이 아니며, 이를 세가지 유형으로 나눌 수 있다. 

1. Missing not at random(MNAR)
해당 value에 의해  밝혀지지 않은 경우
해당 값이 낮거나 높아서 밝히기 꺼려지는 경우에 해당한다.(월급 정보 등)
2. Missing at random(MAR)
다른 관찰 가능한 variable에 의해 밝혀지지 않는 경우
데이터를 잘 살펴보면 특정 값을 지니는 데이터에서만 특정 col이 누락되는 경우에 해당한다.(특정 성별에 따른 나이 공개 여부)
3. Missing completely at random(MCAR)
값이 밝혀지지 않은 이유가 존재하지 않는 경우
그러나 이런 경우는 자주 발생하지 않는다. 

위와 같이 비어 있는 값이 발견되었을 때, 특정한 값을 집어 넣거나, 해당 값을 지우는 선택을 할 수 있다.  

#### Deletion
삭제는 상대적으로 더 쉬운 방법이다. 

해당 field의 값이 많이 비어있는 경우, field 자체(column)를 사용하지 않는 방안이 있다.
그러나 이러한 방법을 활용하게 되면 중요한 정보를 지울 수도 있으며, 이는 모델의 성능 저하를 야기한다. 

다른 방법으로는 비어 있는 데이터(row)를 지우는 방법이 있다. 
이는 비어 있는 값이 많지 않거나, 이유없이 비어있는 데이터(MCAR)의 경우 잘 동작하는 방법이다. 
그러나 특정 이유에 의해 비어있는 데이터(MNAR)의 경우 삭제를 하게되면, 중요한 정보가 지워져 모델 성능 저하를 야기할 수 있다.
또한 다른 데이터에 의해 비어있는 데이터(MAR)의 경우에 모델에 bias를 야기할 수 있다. 

#### Imputation
삭제를 하게되면 중요한 정보를 잃거나 모델에게 bias를 만들 수 있고, 이를 해결하기 위해 특정 값으로 비어있는 곳을 채워줄 수 있다.
그러나 "특정 값"을 지정하는 것은 어려운 일이다. 
일반적인 접근 방법은 ""과 같은 기본 값으로 채워넣는 것이다.
다른 방법으로는 mean, median, most common value 로 채워넣는 방법이 있다.
그러나 이러한 방법들로 인해 실제 값과 구분이 안되거나 기존 모델이 대응하지 못하는 값을 집어넣는 경우 등이 발생할 수 있다.
이러한 경우는 오히려 bias를 집어넣거나, 데이터를 망쳐 노이즈를 만들어주게 된다.  

중요한 것은 __"missing value"를 _완벽하게_ 대응하는 방법은 없다.__ 는 사실이다.

### Scaling
모델이 활용하고자 하는 각각의 feature의 값들이 의미하는 바는 다르다. 
따라서 모델의 입력으로 활용하기전에 비슷한 range로 맞춰주는 것이 중요하고, 이를 feature scaling이라고 한다. 
scaling은 주로 model의 성능을 빠르게 끌어올릴 수 있는 가장 기초적인 방법이다.
min, max를 활용하여 값을 [0, 1]로 만들어주는 방법을 활용하거나, 
해당 feature의 분포가 정규분포를 따른다고 생각되면 normalize 해주는 방식(standaraization)을 활용해 볼 수 있다.

현실에서 ML 모델을 feature의 편향된 분포에 고통을 받는다. 
이런 경우에는 값들에 log를 적용하는 log transforamtion을 적용해볼 수 있다. 
그러나 이런 경우가 항상 잘 동작하는 것이 아니며, log를 씌워 적용한 경우 분석도 log를 씌운 값 위에서 해야한다.
scaling은 data leakage의 보편적인 소스가 되는 문제가 있고, 
또 scaling은 주로 전체 데이터에 대한 통계를 활용하고자 하기 때문에 학습에 이용한 통계를 추론할 때 넘겨 주어야한다.

### Discretization

본 책에서는 방법들을 설명하기 위해 해당 내용을 포함시켜 놓았으나, 현실 적용에서 Discretization이 도움이 되는 경우가 드물다. 
Discretization은 연속 값을 이산값으로 변경하는 과정이다(qunatization이나 binning도 이에 속한다). 
다
연속 값뿐만 아니라 이산 값들도 역시 그룹으로 묶어 나눌 수 있다.
각 그룹의 구분 선을 규정하는 것은 어렵기 때문에 히스토그램을 그려보고 결정하는 등과 같은 선택을 할 수 있다.

### Encoding Categorical Features

카테고리는 시간이 지남에 따라 변하지 않을 것이라 생각하지만, 실제 프로덕션에서는 카테고리가 변한다. 
브랜드를 카테고리로 만들어 관리하게 되면, 새로운 브랜드가 추가되는 경우 문제가 발생할 수있다. 
이에 대응하기 위해 인지도 상위 99%의 브랜드를 인코딩하고 나머지를 unkown으로 지정하는 방법을 활용해 볼 수 있다.
그러나 이 경우에도 마찬가지로 새로운 브랜드가 생성될 때마가 같은 브랜드로 취급받을 수 있는 문제점이 있다. 
이 문제를 해결하는 방법을 찾는 것은 매우 어렵다. 

한가지 솔루션으로는 hashing trick을 활용할 수 있다. 
해쉬를 활용할 경우, 해쉬 공간을 미리 명확하게 지정할 수 있기 때문에 새로운 데이터가 들어오더라도 매핑을 해줄 수 있는 장점이 있다. 
해쉬를 활용하는 경우에는 해쉬 공간의 충돌의 문제로 다른 브랜드가 같은 브랜드로 취급될 수 있으나, 대부분의 hash 함수에서는 충돌이 랜덤하게 발생하고 기존 unkown으로 표기하는 것보다 더 좋은 성능을 보인다.
해쉬 공간이 큰 해쉬함수를 활용하거나, locality-sensitive hashing 과 같은 함수를 활용해서 비슷한 분류를 비슷한 해쉬 값으로 만들어 줄 수도 있다.  

### Feature Crossing
feature crossing은 두 개 이상의 feature를 합쳐 하나의 새로운 feature를 만드는 방법이다. 
각 feature간 비 선형적 관계를 모델링할 때 좋은 방법이다.
linear regression이나 logistic regression, tree-based model에서는 비선형적 요소를 학습하기 어렵기 때문에 feature crossing을 활용하면 더 많은 효과를 볼 수 있다. 

그러나 feature crossing을 활용하는 경우 너무 많은 feature를 도래할 수 있고, 이로인해 모델이 training data에 overfit할 수 있다.

### Discrete and Continuius Positional Embeddings
어텐션의 소개 이후, positional embedding이 NLP와 vision에서 기본적으로 활용되는 기법이 되었다.
positional embedding을 하는 방법은 word embedding을 하는 것과 같은 방법을 활용한다. 
embedding matrix의 columns의 크기로 단어의 수를 각 col은 col속 인덱스를 의미하는 것과 같이 활용할 수 있다.
문장 속 단어의 위치와 같은 discrete position에 대한 positional Embedding은 Fourier feature의 특수 케이스이다.
position이 3차원 오브젝트와 같이 연속적인 수로 지정이 되어 있을때 sine과 cosine을 활용해 fixed position embedding으로 만들어 활용할 수  있다. 

## Data Leakage
### Common Causes for Data Leakage
#### Splitting time-correlated data randomly instead of by time
#### scaling before splitting
#### Filling in missing data with statistics from the test split
#### Poor handling of data duplication before splitting
#### Group leakage
#### Leakage from data generation process

### Detecting Data Leakage

## Enigneering Good Features
### Feature Importance
### Feature Generalization
